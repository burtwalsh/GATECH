
****ANOVA
Great ANOVA explanation with variance in datasets and closeness of means
https://en.wikipedia.org/wiki/Analysis_of_variance#targetText=Analysis%20of%20variance%20(ANOVA)%20is,and%20evolutionary%20biologist%20Ronald%20Fisher.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5296382/

# load libraries
library(datasets)
library(tidyverse) -- has purrr with map

## -- Attaching packages -----------------------------------------------------
- tidyverse 1.2.1 --
## v tibble 2.1.3 v purrr 0.3.2
## v tidyr 0.8.3 v dplyr 0.8.3
## v readr 1.3.1 v stringr 1.4.0
## v tibble 2.1.3 v forcats 0.4.0

ggplot(iris, aes(Petal.Length, Petal.Width, color= Species)) + geom_point()

In kmeans you have to NORMALIZE and randomize your data

# randomize data since it was originaly sorted in order
iris_copy <- iris[order(gp),]
# create a normalize function in order to reduce bias
normalize <- function(x) {
+ return ((x-min(x)) / (max(x)-min(x)))
}

https://datascience.stackexchange.com/questions/14693/what-is-the-difference-of-r-squared-and-adjusted-r-squared#targetText=The%20adjusted%20R%2Dsquared%20is,less%20than%20expected%20by%20chance.
R2 assumes that every single variable explains the variation in the dependent variable. The adjusted R2 tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable.

COEFFICENT INTERPRETATION (Linear regression)
https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/how-to/fit-regression-model/interpret-the-results/all-statistics-and-graphs/coefficients-table/
*****Bus***example INTERACTION EFFECTS -- https://statisticsbyjim.com/regression/interaction-effects/

SE Coef
The standard error of the coefficient estimates the variability between coefficient estimates that you would obtain if you took samples from the same population again and again. The calculation assumes that the sample size and the coefficients to estimate would remain the same if you sampled again and again.

Use the standard error of the coefficient to measure the precision of the estimate of the coefficient. The smaller the standard error, the more precise the estimate. Dividing the coefficient by its standard error calculates a t-value. If the p-value associated with this t-statistic is less than your significance level, you conclude that the coefficient is statistically significant.

T-value
The t-value measures the ratio between the coefficient and its standard error (like standard deviation)

Interaction effects explanation --- https://statisticsbyjim.com/regression/interaction-effects/


https://www.statisticshowto.datasciencecentral.com/p-value/#targetText=The%20p%20value%20is%20the,value%20of%200.0254%20is%202.54%25.
P-value
What if I Don’t Have an Alpha Level?
In an ideal world, you’ll have an alpha level. But if you do not, you can still use the following rough guidelines in deciding whether to support or reject the null hypothesis:

If p > .10 → “not significant”
If p ≤ .10 → “marginally significant”
If p ≤ .05 → “significant”
If p ≤ .01 → “highly significant.”
i
https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/regression-models/model-reduction/
Example of ****REDUCING A MODEL**** in a simple case
Technicians measure the total heat flux as part of a solar thermal energy test. An energy engineer wants to determine how total heat flux is predicted by other variables: insolation, the position of the focal points in the east, south, and north directions, and the time of day. Using the full regression model, the engineer determines the following relationship between heat flux and the variables.


Regression Equation

Heat Flux = 325.4 + 2.55 East + 3.80 South - 22.95 North + 0.0675 Insolation
            + 2.42 Time of Day

Coefficients

Term           Coef  SE Coef  T-Value  P-Value   VIF
Constant      325.4     96.1     3.39    0.003
East           2.55     1.25     2.04    0.053  1.36
South          3.80     1.46     2.60    0.016  3.18
North        -22.95     2.70    -8.49    0.000  2.61
Insolation   0.0675   0.0290     2.33    0.029  2.32
Time of Day    2.42     1.81     1.34    0.194  5.37
The engineer wants to eliminate as many insignificant terms as possible to maximize the precision of predictions. The engineer decides to use 0.05 as the threshold for statistical significance. The p-value for Time of Day (0.194) is the highest p-value that is greater than 0.05, so the engineer removes this term first. The engineer repeats the regression, removing one insignificant term each time, until only statistically significant terms remain. The final reduced model is as follows:


Regression Equation

Heat Flux = 483.7 + 4.796 South - 24.22 North

Coefficients

Term        Coef  SE Coef  T-Value  P-Value   VIF
Constant   483.7     39.6    12.22    0.000
South      4.796    0.951     5.04    0.000  1.09
North     -24.22     1.94   -12.48    0.000  1.09

**VIF
Multicollinearity
Multicollinearity in regression is a condition that occurs when some predictor variables in the model are correlated with other predictor variables. Severe multicollinearity is problematic because it can increase the variance of the regression coefficients, making them unstable. When you remove a term that has high multicollinearity, the statistical significance and values of the coefficients of highly correlated terms can change considerably. 

********************* BEST **********************
https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/regression-models/model-reduction/
****Example of how multicollinearity interferes with the statistical significance criterion****


A team at a medical facility develops a model to predict patient satisfaction scores. The model has several variables, including the time patients are with a practitioner and the time patients are in medical tests. With both of these variables in the model, the multicollinearity is high, with VIF (variance inflation factor) values of 8.91. Values greater than 5 usually indicate severe multicollinearity. The p-value for the amount of time that patients are with a practitioner is 0.105, which is not significant at the 0.05 level. The predicted R2 value for this model is 22.9%.

Regression Analysis: Satisfaction versus Practitioner Time, Test Time


Model Summary

       S    R-sq  R-sq(adj)  R-sq(pred)
0.951953  28.68%     25.64%      22.91%

Coefficients

Term                 Coef  SE Coef  T-Value  P-Value   VIF
Constant           -0.078    0.156    -0.50    0.618
Practitioner Time  0.1071   0.0648     1.65    0.105  8.91
Test Time          -0.516    0.178    -2.90    0.006  8.91

The predicted R2 value for the model with only test time drops from 22.9% to 10.6%. Although the time patients are with a practitioner is not statistically significant at the 0.05 level, including that variable more than doubles the predicted R2 value. The high multicollinearity could be hiding the importance of the predictor.

Regression Analysis: Satisfaction versus Test Time


Model Summary

       S    R-sq  R-sq(adj)  R-sq(pred)
0.968936  24.54%     22.96%      10.61%

Coefficients

Term          Coef  SE Coef  T-Value  P-Value   VIF
Constant    -0.162    0.150    -1.08    0.285
Test Time  -0.2395   0.0606    -3.95    0.000  1.00



********************* BEST **********************

REAL good description of interaction effects
https://statisticsbyjim.com/regression/interaction-effects/
and INTERACTION diagram

R^2
https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit
How close points are to regression line

